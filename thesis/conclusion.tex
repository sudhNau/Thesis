\section{Conclusions and discussion}
In this section we briefly mention the body of literary work directly adjacent to our contribution and later, we summarize the results and conclusions of this thesis. Finally we also mention few immediate pointers to the future works the thesis might lead to.

\subsection{Related Work}
Model proposed in this thesis assumes that the observations as well as their labels were generated by a linear combination of multiple latent processes. [Luttinen] used a similar approach in probabilistic factor analysis case to model spatio-temporal datasets by using Gaussian process priors for both mixing matrix as well as the latent components corresponding to time signals.  [GPRN] in their model GPRN combine the ideas of Bayesian neural network structure by making the mixing matrix input dependent Gaussian process and thereby introducing non linearity in the mixture as well and created a general purpose multi tasking framework. [Sami’s paper] further extends the idea by proposing a novel non stationary Latent Correlational Kernel that creates a shared correlation structure between the input dependent latent processes. Both regression and classification tasks on multi observational sets are handeled by [Sami’s paper]. 

Another direction of research is the introduction of deeper hierarchy in the latent part of the model. [Neil] proposes an extension to GP-latent variable model which can scale [GPLVM] vertically by encoding multiple layers of latent spaces as well as horizontally by introducing additional conditional independencies between latent variables. 

\subsection{Conclusions}
To summarize, our main objective in this thesis was to present a latent Gaussian processes classifier that is capable of ingesting multivariate data streams for each instance and categorize them in separate classes by extracting the relevant information to a low dimensional latent space. Gaussian processes enable a rich non linear latent space for the data to be projected into, while a sparse solution alleviates the scalability issues that may creep into when using GPs.

Proposed model was tested as a single class classifier in two different experiment settings, first using an artificial dataset where positive instances consisted of a linear trend mixed with other Gaussian trend to test model’s performance with different parameter scenarios like varying amount of output dimensions, induction ratio etc. The results presented in section 3.3.1 of this experiment conclude that the model is stable and classifier performance was consistent. Second experiment was performed using an open source benchmark dataset for time series from UCI, to test model’s performance in a more real setting and compare it with other classification methods. Results of this experiment as explained in section 3.3.2 were also very promising and demonstrated that the performance of LCGC with a default settings was comparable to the open source implementation of other state of the art classification methods like Logistic regression, SVM etc. Moreover, being a fully Bayesian model, using LCGC also means that uncertainty information is readily available at every stage of inference be it latent processes or latent predicted values something that’s not possible with above mentioned models.

In addition to proposing LCGC, we also extended SLFM model proposed by [Seeger et al] by enabling it to handle multi-observational scenarios such that each observation has it's own latent space and proposing a sparse variational approximation both to original model as well as our extension of it. As explained in section 1.3, computational complexity of inference in Gaussian process grows with an order of three which can make working with large dataset challenging. Sparse solution thus makes the model more scalable and efficient. Visual demonstrations of these models on artificially generated datasets provided in section 2.2.3 and 2.3.2 respectively indicate that the sparse models are able to satisfactorily capture the latent processes of data even with a 0.5 to 0.7 induction ratio.

The experiments and demonstrations in this thesis were performed only on artificially generated or simulated datasets, hence an immediate future continuation of this work might include the tests and experiments based on real world datasets. Apart from classical FMRI, share recognition datasets many new datasets should be expected following the recent explosion of data collection in IoT as well as wearable device and medical areas. Moreover, all of the experiments the naïve LCGC model was used with default Gaussian kernels for latent processes. In real world however, there is always much more information about the supposed latent structure is available. This information can be encoded in the model by using different types of Kernel in the latent processes, for example a periodic or Brownian model might provide better results if the data is known to have cycles or trends structures. Similarly, performing hyper-parameter optimization before variational algorithm should also help improve the performance of LCGC model in real world data analysis problems. 
